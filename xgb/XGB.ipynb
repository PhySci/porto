{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ecf01439-91f9-40c2-90ca-cfc9fef897f9",
    "_uuid": "663762d0acd501a442290010ddc25eaea219ff13"
   },
   "source": [
    "Based on [olivier's script](https://www.kaggle.com/ogrellier/xgb-classifier-upsampling-lb-0-283)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "45ba73d4-6c4c-40bb-9390-7dfc956c555d",
    "_uuid": "dbd332f83c89108c4e641218f5c4e7b9cd325b80"
   },
   "outputs": [],
   "source": [
    "EARLY_STOPPING_ROUNDS = 50  \n",
    "# Note: I set EARLY_STOPPING_ROUNDS high so that (when OPTIMIZE_ROUNDS is set)\n",
    "#       I will get lots of information to make my own judgment.  You should probably\n",
    "#       reduce EARLY_STOPPING_ROUNDS if you want to do actual early stopping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7e199c98-16b0-45e7-a6d1-bdd9325c2631",
    "_uuid": "b277fe426336d65ac71f0e6ac96c7ee16d02074c"
   },
   "source": [
    "I recommend initially setting <code>MAX_ROUNDS</code> fairly high and using <code>OPTIMIZE_ROUNDS</code> to get an idea of the appropriate number of rounds (which, in my judgment, should be close to the maximum value of  <code>best_ntree_limit</code> among all folds, maybe even a bit higher if your model is adequately regularized...or alternatively, you could set <code>verbose=True</code> and look at the details to try to find a number of rounds that works well for all folds).  Then I would turn off <code>OPTIMIZE_ROUNDS</code> and set <code>MAX_ROUNDS</code> to the appropraite number of total rounds.  \n",
    "\n",
    "The problem with \"early stopping\" by choosing the best round for each fold is that it overfits to the validation data.    It's therefore liable not to produce the optimal model for predicting test data, and if it's used to produce validation data for stacking/ensembling with other models, it would cause this one to have too much weight in the ensemble.  Another possibility (and the default for XGBoost, it seems) is to use the round where the early stop actually happens (with the lag that verifies lack of improvement) rather than the best round.  That solves the overfitting problem (provided the lag is long enough), but so far it doesn't seem to have helped.  (I got a worse validation score with 20-round early stopping per fold than with a constant number of rounds for all folds, so the early stopping actually seemed to underfit.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b7258128-55f9-4543-8611-5e0a6661837b",
    "_uuid": "72171ee53e170096d37a18eef84682fa348ae5c4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/frodos/ML/local/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from numba import jit\n",
    "import time\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "3d16f16e-12cc-4b41-b7bd-fa05ce44770c",
    "_uuid": "154b078a7e86c0a5a328118a61d28e2581bb3b0a"
   },
   "outputs": [],
   "source": [
    "# Compute gini\n",
    "\n",
    "# from CPMP's kernel https://www.kaggle.com/cpmpml/extremely-fast-gini-computation\n",
    "@jit\n",
    "def eval_gini(y_true, y_prob):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_true = y_true[np.argsort(y_prob)]\n",
    "    ntrue = 0\n",
    "    gini = 0\n",
    "    delta = 0\n",
    "    n = len(y_true)\n",
    "    for i in range(n-1, -1, -1):\n",
    "        y_i = y_true[i]\n",
    "        ntrue += y_i\n",
    "        gini += y_i * delta\n",
    "        delta += 1 - y_i\n",
    "    gini = 1 - 2 * gini / (ntrue * (n - ntrue))\n",
    "    return gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini(actual, pred, cmpcol = 0, sortcol = 1):\n",
    "    all = np.asarray(np.c_[ actual, pred, np.arange(len(actual)) ], dtype=np.float)\n",
    "    all = all[ np.lexsort((all[:,2], -1*all[:,1])) ]\n",
    "    totalLosses = all[:,0].sum()\n",
    "    giniSum = all[:,0].cumsum().sum() / totalLosses\n",
    "    giniSum -= (len(actual) + 1) / 2.\n",
    "    return giniSum / len(actual)\n",
    " \n",
    "def gini_normalized(a, p):\n",
    "    return gini(a, p) / gini(a, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "99b88ea4-a9af-45aa-9df0-86412d7264cf",
    "_uuid": "67a8ca9dead7110c776d7f75bb8963b3429617cb"
   },
   "outputs": [],
   "source": [
    "# Functions from olivier's kernel\n",
    "# https://www.kaggle.com/ogrellier/xgb-classifier-upsampling-lb-0-283\n",
    "\n",
    "def gini_xgb(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    gini_score = -eval_gini(labels, preds)\n",
    "    return [('gini', gini_score)]\n",
    "\n",
    "\n",
    "def add_noise(series, noise_level):\n",
    "    return series * (1 + noise_level * np.random.randn(series.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData():\n",
    "        # from olivier\n",
    "    train_features = [\n",
    "        \"ps_car_13\",  #            : 1571.65 / shadow  609.23\n",
    "        \"ps_reg_03\",  #            : 1408.42 / shadow  511.15\n",
    "        \"ps_ind_05_cat\",  #        : 1387.87 / shadow   84.72\n",
    "        \"ps_ind_03\",  #            : 1219.47 / shadow  230.55\n",
    "        \"ps_ind_15\",  #            :  922.18 / shadow  242.00\n",
    "        \"ps_reg_02\",  #            :  920.65 / shadow  267.50\n",
    "        \"ps_car_14\",  #            :  798.48 / shadow  549.58\n",
    "        \"ps_car_12\",  #            :  731.93 / shadow  293.62\n",
    "        \"ps_car_01_cat\",  #        :  698.07 / shadow  178.72\n",
    "        \"ps_car_07_cat\",  #        :  694.53 / shadow   36.35\n",
    "        \"ps_ind_17_bin\",  #        :  620.77 / shadow   23.15\n",
    "        \"ps_car_03_cat\",  #        :  611.73 / shadow   50.67\n",
    "        \"ps_reg_01\",  #            :  598.60 / shadow  178.57\n",
    "        \"ps_car_15\",  #            :  593.35 / shadow  226.43\n",
    "        \"ps_ind_01\",  #            :  547.32 / shadow  154.58\n",
    "        \"ps_ind_16_bin\",  #        :  475.37 / shadow   34.17\n",
    "        \"ps_ind_07_bin\",  #        :  435.28 / shadow   28.92\n",
    "        \"ps_car_06_cat\",  #        :  398.02 / shadow  212.43\n",
    "        \"ps_car_04_cat\",  #        :  376.87 / shadow   76.98\n",
    "        \"ps_ind_06_bin\",  #        :  370.97 / shadow   36.13\n",
    "        \"ps_car_09_cat\",  #        :  214.12 / shadow   81.38\n",
    "        \"ps_car_02_cat\",  #        :  203.03 / shadow   26.67\n",
    "        \"ps_ind_02_cat\",  #        :  189.47 / shadow   65.68\n",
    "        \"ps_car_11\",  #            :  173.28 / shadow   76.45\n",
    "        \"ps_car_05_cat\",  #        :  172.75 / shadow   62.92\n",
    "        \"ps_calc_09\",  #           :  169.13 / shadow  129.72\n",
    "        \"ps_calc_05\",  #           :  148.83 / shadow  120.68\n",
    "        \"ps_ind_08_bin\",  #        :  140.73 / shadow   27.63\n",
    "        \"ps_car_08_cat\",  #        :  120.87 / shadow   28.82\n",
    "        \"ps_ind_09_bin\",  #        :  113.92 / shadow   27.05\n",
    "        \"ps_ind_04_cat\",  #        :  107.27 / shadow   37.43\n",
    "        \"ps_ind_18_bin\",  #        :   77.42 / shadow   25.97\n",
    "        \"ps_ind_12_bin\",  #        :   39.67 / shadow   15.52\n",
    "        \"ps_ind_14\",  #            :   37.37 / shadow   16.65\n",
    "    ]\n",
    "    \n",
    "    # Read data\n",
    "    train_df = pd.read_csv('../data/train.csv', na_values=\"-1\", index_col='id') # .iloc[0:200,:]\n",
    "    test_df = pd.read_csv('../data/test.csv', na_values=\"-1\", index_col='id')\n",
    "    \n",
    "    train_df = train_df.loc[:, train_features + [\"target\"]]\n",
    "    test_df  = test_df.loc[:,train_features]\n",
    "    return [train_df, test_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "52b50086-b405-4598-b11c-97887cdcce8e",
    "_uuid": "07a5a5782894611e9006ae1b399b0b8fb8a0f06b"
   },
   "outputs": [],
   "source": [
    "[train_df, test_df] = getData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class featureCombination:\n",
    "    \"\"\"\n",
    "    Class provide usual \"fit-tranform\" functionality for feature combinations \n",
    "    \"\"\"\n",
    "    def __init__(self, combinationList):\n",
    "        self.comninationList = combinationList\n",
    "        self.encoders = dict()\n",
    "    \n",
    "    def fit_transform(self,df):\n",
    "        X = df.copy() \n",
    "        \n",
    "        for n_c, (f1, f2) in enumerate(self.comninationList):\n",
    "            fName = f1 + \"_plus_\" + f2\n",
    "            # add new feature\n",
    "            X.insert(X.shape[1], fName, X[f1].astype('str')+\"_\"+X[f2].astype('str'))\n",
    "                     \n",
    "            # encode new feature         \n",
    "            lbl = LabelEncoder()\n",
    "            X[fName] = lbl.fit_transform(X[fName])\n",
    "            self.encoders.update({fName:lbl})\n",
    "               \n",
    "        return X\n",
    "    \n",
    "    def transform(self,df):\n",
    "        X = df.copy() \n",
    "        \n",
    "        for n_c, (f1, f2) in enumerate(self.comninationList):\n",
    "            fName = f1 + \"_plus_\" + f2\n",
    "            # add new feature\n",
    "            X.insert(X.shape[1], fName, X[f1].astype('str')+\"_\"+X[f2].astype('str'))\n",
    "                     \n",
    "            # encode new feature         \n",
    "            lbl = self.encoders.get(fName)\n",
    "            X[fName] = lbl.fit_transform(X[fName])\n",
    "            self.encoders.update({fName:lbl})\n",
    "               \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class targetEncode():\n",
    "    \n",
    "    \n",
    "    def __init__(self, features, min_samples_leaf=1, smoothing=1, fillna = True):\n",
    "        self.suffix = '_tarEncd'\n",
    "        \n",
    "        self.features = features\n",
    "        \n",
    "        self.min_samples_leaf = 1\n",
    "        self.smoothing = smoothing\n",
    "        \n",
    "        self.encoders = dict()\n",
    "        self.prior = 1.0\n",
    "        self.fillna = fillna\n",
    "        \n",
    "        \n",
    "    def fit_transform(self, inpDf, targetColumn = 'target'):\n",
    "        \"\"\"\n",
    "        Fit to features and transform them\n",
    "        params:\n",
    "            df - dataframe to process\n",
    "            targetColumn  - name of columns which will be used as a target variable \n",
    "        \"\"\"\n",
    "\n",
    "        for colName in self.features:\n",
    "            \n",
    "            if not (colName in inpDf.columns):\n",
    "                raise Exception('Columns name mismatch', 'Column %s was not found in dataframe'%colName)\n",
    "            \n",
    "            self.prior = inpDf[targetColumn].mean()\n",
    "            averages = inpDf.groupby(by= colName)[targetColumn].agg([\"mean\", \"count\"])\n",
    "            smoothing = 1 / (1 + np.exp(-(averages[\"count\"] - self.min_samples_leaf) / self.smoothing))\n",
    "            averages['target'] = self.prior * (1 - smoothing) + averages[\"mean\"] * smoothing\n",
    "            self.encoders.update({colName: averages.to_dict(orient= 'dict')['target']})\n",
    "            \n",
    "        return self.transform(inpDf)\n",
    "    \n",
    "    def transform(self, inpDf):\n",
    "        \"\"\"\n",
    "        Transform features\n",
    "        params:\n",
    "            df - dataframe to process\n",
    "        \"\"\"\n",
    "        \n",
    "        df = inpDf.copy()    \n",
    "        for colName in self.features:\n",
    "            if colName+self.suffix in df.columns:\n",
    "                df.drop(colName+self.suffix, axis = 1, inplace= True)\n",
    "            \n",
    "            ts = df[colName].map(self.encoders[colName], na_action= 'ignore')\n",
    "            \n",
    "            if self.fillna:\n",
    "                ts.fillna(self.prior, inplace = True)\n",
    "            \n",
    "            df.insert(df.shape[1], colName+self.suffix, ts)\n",
    "        return df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up classifier\n",
    "def getXGBModel(params,seed = 43):\n",
    "    return XGBClassifier(seed = seed, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "d9a217fa-50f4-43a7-805b-d1c796a7ebf7",
    "_uuid": "da09aaf7c0c77a131c7d9d53feae512c8f9730c1"
   },
   "outputs": [],
   "source": [
    "# add combinations\n",
    "combs = [('ps_reg_01', 'ps_car_02_cat'),  \n",
    "         ('ps_reg_01', 'ps_car_04_cat')]\n",
    "\n",
    "fComb = featureCombination(combs)\n",
    "\n",
    "train_df = fComb.fit_transform(train_df)\n",
    "test_df = fComb.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTIMIZE_ROUNDS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_cats = [x for x in test_df.columns if x.endswith('_cat')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit(predDf, fName = 'submission.csv'):\n",
    "    \"\"\"\n",
    "    Write submission file.\n",
    "    params:\n",
    "        pred - predictions (array)\n",
    "        ind - index for prediction (array)\n",
    "        fName - name of file (string)\n",
    "    \"\"\"\n",
    "    predDf.rename(columns={'prediction':'target'},inplace=True)\n",
    "    predDf.to_csv(fName,index_label='id',float_format='%.6f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainXGB(params, varSeed= True, folds= 5, encode_target= True):\n",
    "    # Run CV\n",
    "    localScore = []\n",
    "    \n",
    "    valDf = pd.DataFrame(0, index = train_df.index, columns = ['fold','prediction'])\n",
    "    testDf = pd.DataFrame(0, index = test_df.index, columns = ['prediction'])\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\n",
    "    for i, (train_index, test_index) in enumerate(skf.split(train_df.drop(\"target\", axis= 1), train_df.target)):\n",
    "\n",
    "        print \"\\nFold \", i\n",
    "\n",
    "        # Create data for this fold\n",
    "        if encode_target:\n",
    "            te = targetEncode(features= f_cats, min_samples_leaf= 200, smoothing= 100)\n",
    "            X_train = te.fit_transform(train_df.iloc[train_index,:])\n",
    "            X_valid = te.transform(train_df.iloc[test_index,:])\n",
    "            X_test  = te.transform(test_df)\n",
    "        else:\n",
    "            X_train = train_df.iloc[train_index,:]\n",
    "            X_valid = train_df.iloc[test_index,:]\n",
    "            X_test  = test_df\n",
    "\n",
    "        model = getXGBModel(params, i if varSeed else 0)\n",
    "        model.fit(X_train.drop(\"target\", axis= 1), X_train.target, \n",
    "                   eval_set= [(X_valid.drop(\"target\", axis= 1), X_valid.target)],\n",
    "                   eval_metric= 'auc',\n",
    "                   early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n",
    "                   verbose=False)\n",
    "\n",
    "        # Generate validation predictions for this fold\n",
    "        pred = model.predict_proba(X_valid.drop(\"target\", axis= 1))[:,1]\n",
    "        valDf.prediction.iloc[test_index] = pred\n",
    "        valDf.fold.iloc[test_index] = i\n",
    "\n",
    "        ls = 2*roc_auc_score(X_valid.target, pred)-1\n",
    "        localScore.append(ls)\n",
    "\n",
    "        print \"  Best N trees = \", model.best_ntree_limit\n",
    "        print \"  Best ROC-AUC = \", 2*model.best_score-1\n",
    "        print \"  Validation score is %f\"%ls\n",
    "\n",
    "        testDf['prediction']+= model.predict_proba(X_test)[:,1]\n",
    "\n",
    "    testDf['prediction'] /= folds  # Average test set predictions\n",
    "    localScore =np.array(localScore)\n",
    "\n",
    "    print \"\\nGini for full training set (AUC): %f\"%(2*roc_auc_score(train_df.target, valDf.prediction)-1)\n",
    "    print \"\\nLocal score is %f, std is %f\"%(localScore.mean(), localScore.std())\n",
    "        \n",
    "    return {'test_prediction':testDf, 'localScore':localScore, 'train_prediction': valDf}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "_cell_guid": "c4e48347-920f-4ba7-8b37-cfbaab4c3c00",
    "_uuid": "2b9ed96c98b705d3e4bf2a3d60323dfab4332674",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xgbParams = {'n_estimators': 1000,\n",
    "             'max_depth': 4,\n",
    "             'objective': \"binary:logistic\",\n",
    "             'learning_rate': 0.07,\n",
    "             'subsample': 0.8,\n",
    "             'min_child_weight': 6,\n",
    "             'colsample_bytree': .8,\n",
    "             'scale_pos_weight': 1.6,\n",
    "             'gamma': 10,\n",
    "             'reg_alpha': 8,\n",
    "             'reg_lambda': 1.3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold  0\n",
      "  Best N trees =  482\n",
      "  Best ROC-AUC =  0.270762\n",
      "  Validation score is 0.270367\n",
      "\n",
      "Fold  1\n",
      "  Best N trees =  356\n",
      "  Best ROC-AUC =  0.294946\n",
      "  Validation score is 0.294701\n",
      "\n",
      "Fold  2\n",
      "  Best N trees =  226\n",
      "  Best ROC-AUC =  0.28695\n",
      "  Validation score is 0.286391\n",
      "\n",
      "Fold  3\n",
      "  Best N trees =  462\n",
      "  Best ROC-AUC =  0.296286\n",
      "  Validation score is 0.296028\n",
      "\n",
      "Fold  4\n",
      "  Best N trees =  284\n",
      "  Best ROC-AUC =  0.28902\n",
      "  Validation score is 0.288700\n",
      "\n",
      "Gini for full training set (AUC): 0.287122\n",
      "\n",
      "Local score is 0.287237, std is 0.009170\n"
     ]
    }
   ],
   "source": [
    "model1 = trainXGB(xgbParams, encode_target= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold  0\n",
      "  Best N trees =  567\n",
      "  Best ROC-AUC =  0.2709\n",
      "  Validation score is 0.270641\n",
      "\n",
      "Fold  1\n",
      "  Best N trees =  324\n",
      "  Best ROC-AUC =  0.29263\n",
      "  Validation score is 0.292186\n",
      "\n",
      "Fold  2\n",
      "  Best N trees =  383\n",
      "  Best ROC-AUC =  0.286054\n",
      "  Validation score is 0.285971\n",
      "\n",
      "Fold  3\n",
      "  Best N trees =  302\n",
      "  Best ROC-AUC =  0.290518\n",
      "  Validation score is 0.290311\n",
      "\n",
      "Fold  4\n",
      "  Best N trees =  178\n",
      "  Best ROC-AUC =  0.28382\n",
      "  Validation score is 0.283716\n",
      "\n",
      "Gini for full training set (AUC): 0.284444\n",
      "\n",
      "Local score is 0.284565, std is 0.007585\n"
     ]
    }
   ],
   "source": [
    "model2 = trainXGB(xgbParams, encode_target= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.042243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.043655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.038021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.022985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.055887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.068505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.024037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.055165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.077307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.090465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.044877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.037109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.067440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.064836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.066381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.034596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.035048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.082590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.022374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.081419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.052637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.084745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.092653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.024338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.037716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.043084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.119234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.058591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.039354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.023617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1487982</th>\n",
       "      <td>0.026253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1487984</th>\n",
       "      <td>0.056722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1487985</th>\n",
       "      <td>0.052887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1487986</th>\n",
       "      <td>0.056850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1487987</th>\n",
       "      <td>0.049981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1487989</th>\n",
       "      <td>0.039880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1487991</th>\n",
       "      <td>0.043809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1487993</th>\n",
       "      <td>0.063342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1487995</th>\n",
       "      <td>0.037474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1487997</th>\n",
       "      <td>0.056764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1487998</th>\n",
       "      <td>0.080317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1487999</th>\n",
       "      <td>0.083690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1488000</th>\n",
       "      <td>0.058324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1488002</th>\n",
       "      <td>0.121311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1488003</th>\n",
       "      <td>0.037964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1488004</th>\n",
       "      <td>0.045170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1488006</th>\n",
       "      <td>0.042267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1488007</th>\n",
       "      <td>0.049429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1488010</th>\n",
       "      <td>0.067630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1488012</th>\n",
       "      <td>0.055136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1488014</th>\n",
       "      <td>0.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1488015</th>\n",
       "      <td>0.038625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1488018</th>\n",
       "      <td>0.037474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1488019</th>\n",
       "      <td>0.023885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1488020</th>\n",
       "      <td>0.032693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1488022</th>\n",
       "      <td>0.157222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1488023</th>\n",
       "      <td>0.060308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1488024</th>\n",
       "      <td>0.058175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1488025</th>\n",
       "      <td>0.036744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1488026</th>\n",
       "      <td>0.051223</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>892816 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         prediction\n",
       "id                 \n",
       "0          0.042243\n",
       "1          0.043655\n",
       "2          0.038021\n",
       "3          0.022985\n",
       "4          0.055887\n",
       "5          0.068505\n",
       "6          0.024037\n",
       "8          0.055165\n",
       "10         0.077307\n",
       "11         0.090465\n",
       "12         0.044877\n",
       "14         0.037109\n",
       "15         0.067440\n",
       "18         0.064836\n",
       "21         0.066381\n",
       "23         0.034596\n",
       "24         0.035048\n",
       "25         0.082590\n",
       "27         0.022374\n",
       "29         0.081419\n",
       "30         0.052637\n",
       "31         0.084745\n",
       "32         0.092653\n",
       "33         0.024338\n",
       "37         0.037716\n",
       "38         0.043084\n",
       "39         0.119234\n",
       "40         0.058591\n",
       "41         0.039354\n",
       "42         0.023617\n",
       "...             ...\n",
       "1487982    0.026253\n",
       "1487984    0.056722\n",
       "1487985    0.052887\n",
       "1487986    0.056850\n",
       "1487987    0.049981\n",
       "1487989    0.039880\n",
       "1487991    0.043809\n",
       "1487993    0.063342\n",
       "1487995    0.037474\n",
       "1487997    0.056764\n",
       "1487998    0.080317\n",
       "1487999    0.083690\n",
       "1488000    0.058324\n",
       "1488002    0.121311\n",
       "1488003    0.037964\n",
       "1488004    0.045170\n",
       "1488006    0.042267\n",
       "1488007    0.049429\n",
       "1488010    0.067630\n",
       "1488012    0.055136\n",
       "1488014    0.040000\n",
       "1488015    0.038625\n",
       "1488018    0.037474\n",
       "1488019    0.023885\n",
       "1488020    0.032693\n",
       "1488022    0.157222\n",
       "1488023    0.060308\n",
       "1488024    0.058175\n",
       "1488025    0.036744\n",
       "1488026    0.051223\n",
       "\n",
       "[892816 rows x 1 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2['test_prediction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit(model2['test_prediction'],'xgb5.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.282"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit(model1['test_prediction'],'xgb5.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.281"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "summ = 0.5*(model1['test_prediction']+model2['test_prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit(summ,'xgb6.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.283"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.283"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xgbModel1.dmp']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(model1,'xgbModel1.dmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xgbModel2.dmp']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(model2,'xgbModel2.dmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Machine Learning)",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
